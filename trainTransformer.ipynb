{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/parallelproj/backend.py:18: UserWarning: The numpy.array_api submodule is still experimental. See NEP 47.\n",
      "  from numpy.array_api._array_object import Array\n",
      "/home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import parallelproj\n",
    "import torch\n",
    "import numpy as np\n",
    "from odl.contrib import fom\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from array_api_compat import to_device\n",
    "import array_api_compat.torch as xp\n",
    "from data import *\n",
    "from geometry import *\n",
    "from torch.nn.parallel import DataParallel as DP\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/parallelproj/pet_scanners.py:309: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  side = inds // self.num_lor_endpoints_per_side\n",
      "/home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/parallelproj/pet_lors.py:258: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.xp.concat((self.xp.arange(m) // 2, self.xp.asarray([n // 2])))\n",
      "/home/mamo_alegua/anaconda3/envs/anton2/lib/python3.9/site-packages/parallelproj/pet_lors.py:262: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.xp.concat((self.xp.asarray([-1]), -((self.xp.arange(m) + 4) // 2)))\n"
     ]
    }
   ],
   "source": [
    "proj = get_minipet_projector(dev,num_rings=21)\n",
    "num_train = 500\n",
    "batch_size = 6\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(RandomEllipsoids(num_ellipsoids=np.random.poisson(10), diag=200, proj=proj, num_images=num_train, axes_scale=0.3, dim=3),\n",
    "                                            batch_size, shuffle=True, num_workers=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 30,276,972\n"
     ]
    }
   ],
   "source": [
    "from lpdTransformer99 import *\n",
    "\n",
    "num_epochs = 150\n",
    "n_iter = 3\n",
    "normalisation=(proj.norm(xp,\"cuda\"))**2\n",
    "model = DP(LPD(n_iter, proj, normalisation, indices=patch_sinogram(proj, patch_size=(7,7), return_values=True)), device_ids=[1,2,3,4,6,7])\n",
    "model.to(dev)\n",
    "print(\"Number of parameters: {:,}\".format(sum(p.numel() for p in model.parameters())))\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "no_decay = []\n",
    "decay = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and 'batchnorm' and 'layernorm' and 'relative_bias_table' and 'pixel_position' not in name:\n",
    "        decay.append(param)\n",
    "    else:\n",
    "        no_decay.append(param)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': decay, 'weight_decay': 0.01},\n",
    "    {'params': no_decay, 'weight_decay': 0}\n",
    "], lr=1e-4)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4,\n",
    "                                             steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "scanner = None\n",
    "lor_desc = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp = torch.load(\"/home/mamo_alegua/anaconda3/LPDTransformer/\"+\"3iterTransformerDualV7Sino100\")\n",
    "#model.module.load_state_dict(cp['model'])\n",
    "#optimizer.load_state_dict(cp['optimizer'])\n",
    "#lr_scheduler.load_state_dict(cp['lr_scheduler'])\n",
    "#start_epoch = cp['epoch'] + 1\n",
    "#del cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/150 - loss 0.044; PSNR -inf; SSIM 0.027\n",
      "2/150 - loss 0.015; PSNR -inf; SSIM 0.215\n",
      "3/150 - loss 0.010; PSNR -inf; SSIM 0.293\n",
      "4/150 - loss 0.007; PSNR -inf; SSIM 0.374\n",
      "5/150 - loss 0.006; PSNR -inf; SSIM 0.439\n",
      "6/150 - loss 0.005; PSNR -inf; SSIM 0.482\n",
      "7/150 - loss 0.004; PSNR -inf; SSIM 0.504\n",
      "8/150 - loss 0.005; PSNR -inf; SSIM 0.511\n",
      "9/150 - loss 0.004; PSNR -inf; SSIM 0.529\n",
      "10/150 - loss 0.004; PSNR 23.767; SSIM 0.563\n",
      "11/150 - loss 0.004; PSNR -inf; SSIM 0.578\n",
      "12/150 - loss 0.003; PSNR -inf; SSIM 0.599\n",
      "13/150 - loss 0.003; PSNR -inf; SSIM 0.599\n",
      "14/150 - loss 0.003; PSNR -inf; SSIM 0.626\n",
      "15/150 - loss 0.003; PSNR -inf; SSIM 0.613\n",
      "16/150 - loss 0.003; PSNR -inf; SSIM 0.607\n",
      "17/150 - loss 0.003; PSNR -inf; SSIM 0.677\n",
      "18/150 - loss 0.003; PSNR -inf; SSIM 0.637\n",
      "19/150 - loss 0.003; PSNR -inf; SSIM 0.659\n",
      "20/150 - loss 0.003; PSNR -inf; SSIM 0.645\n",
      "21/150 - loss 0.004; PSNR -inf; SSIM 0.598\n",
      "22/150 - loss 0.003; PSNR -inf; SSIM 0.668\n",
      "23/150 - loss 0.003; PSNR -inf; SSIM 0.651\n",
      "24/150 - loss 0.002; PSNR -inf; SSIM 0.707\n",
      "25/150 - loss 0.002; PSNR -inf; SSIM 0.711\n",
      "Saving Checkpoint...\n",
      "26/150 - loss 0.003; PSNR -inf; SSIM 0.658\n",
      "27/150 - loss 0.002; PSNR -inf; SSIM 0.668\n",
      "28/150 - loss 0.002; PSNR -inf; SSIM 0.731\n",
      "29/150 - loss 0.002; PSNR -inf; SSIM 0.710\n",
      "30/150 - loss 0.002; PSNR -inf; SSIM 0.706\n",
      "31/150 - loss 0.003; PSNR -inf; SSIM 0.674\n",
      "32/150 - loss 0.002; PSNR -inf; SSIM 0.698\n",
      "33/150 - loss 0.002; PSNR -inf; SSIM 0.715\n",
      "34/150 - loss 0.002; PSNR -inf; SSIM 0.723\n",
      "35/150 - loss 0.002; PSNR -inf; SSIM 0.705\n",
      "36/150 - loss 0.002; PSNR -inf; SSIM 0.710\n",
      "37/150 - loss 0.002; PSNR -inf; SSIM 0.737\n",
      "38/150 - loss 0.002; PSNR -inf; SSIM 0.738\n",
      "39/150 - loss 0.003; PSNR -inf; SSIM 0.674\n",
      "40/150 - loss 0.002; PSNR -inf; SSIM 0.697\n",
      "41/150 - loss 0.002; PSNR -inf; SSIM 0.668\n",
      "42/150 - loss 0.002; PSNR -inf; SSIM 0.782\n",
      "43/150 - loss 0.002; PSNR -inf; SSIM 0.773\n",
      "44/150 - loss 0.002; PSNR -inf; SSIM 0.771\n",
      "45/150 - loss 0.002; PSNR -inf; SSIM 0.681\n",
      "46/150 - loss 0.002; PSNR -inf; SSIM 0.745\n",
      "47/150 - loss 0.002; PSNR -inf; SSIM 0.727\n",
      "48/150 - loss 0.002; PSNR -inf; SSIM 0.747\n",
      "49/150 - loss 0.002; PSNR -inf; SSIM 0.756\n",
      "50/150 - loss 0.002; PSNR -inf; SSIM 0.727\n",
      "Saving Checkpoint...\n",
      "51/150 - loss 0.002; PSNR -inf; SSIM 0.804\n",
      "52/150 - loss 0.002; PSNR -inf; SSIM 0.776\n",
      "53/150 - loss 0.002; PSNR -inf; SSIM 0.812\n",
      "54/150 - loss 0.002; PSNR -inf; SSIM 0.778\n",
      "55/150 - loss 0.002; PSNR -inf; SSIM 0.771\n",
      "56/150 - loss 0.001; PSNR -inf; SSIM 0.801\n",
      "57/150 - loss 0.002; PSNR -inf; SSIM 0.711\n",
      "58/150 - loss 0.002; PSNR -inf; SSIM 0.757\n",
      "59/150 - loss 0.002; PSNR -inf; SSIM 0.805\n",
      "60/150 - loss 0.002; PSNR -inf; SSIM 0.804\n",
      "61/150 - loss 0.001; PSNR -inf; SSIM 0.818\n",
      "62/150 - loss 0.002; PSNR -inf; SSIM 0.794\n",
      "63/150 - loss 0.002; PSNR -inf; SSIM 0.792\n",
      "64/150 - loss 0.001; PSNR 27.572; SSIM 0.817\n",
      "65/150 - loss 0.002; PSNR -inf; SSIM 0.790\n",
      "66/150 - loss 0.002; PSNR -inf; SSIM 0.785\n",
      "67/150 - loss 0.002; PSNR -inf; SSIM 0.781\n",
      "68/150 - loss 0.001; PSNR -inf; SSIM 0.794\n",
      "69/150 - loss 0.002; PSNR -inf; SSIM 0.768\n",
      "70/150 - loss 0.002; PSNR -inf; SSIM 0.795\n",
      "71/150 - loss 0.001; PSNR -inf; SSIM 0.798\n",
      "72/150 - loss 0.001; PSNR -inf; SSIM 0.822\n",
      "73/150 - loss 0.001; PSNR -inf; SSIM 0.856\n",
      "74/150 - loss 0.001; PSNR -inf; SSIM 0.849\n",
      "75/150 - loss 0.001; PSNR -inf; SSIM 0.806\n",
      "Saving Checkpoint...\n",
      "76/150 - loss 0.001; PSNR -inf; SSIM 0.805\n",
      "77/150 - loss 0.002; PSNR -inf; SSIM 0.809\n",
      "78/150 - loss 0.001; PSNR -inf; SSIM 0.840\n",
      "79/150 - loss 0.001; PSNR -inf; SSIM 0.795\n",
      "80/150 - loss 0.001; PSNR -inf; SSIM 0.820\n",
      "81/150 - loss 0.001; PSNR -inf; SSIM 0.800\n",
      "82/150 - loss 0.001; PSNR -inf; SSIM 0.820\n",
      "83/150 - loss 0.002; PSNR -inf; SSIM 0.801\n",
      "84/150 - loss 0.001; PSNR -inf; SSIM 0.789\n",
      "85/150 - loss 0.001; PSNR -inf; SSIM 0.797\n",
      "86/150 - loss 0.001; PSNR -inf; SSIM 0.834\n",
      "87/150 - loss 0.001; PSNR -inf; SSIM 0.833\n",
      "88/150 - loss 0.001; PSNR -inf; SSIM 0.836\n",
      "89/150 - loss 0.001; PSNR -inf; SSIM 0.811\n",
      "90/150 - loss 0.001; PSNR -inf; SSIM 0.833\n",
      "91/150 - loss 0.001; PSNR -inf; SSIM 0.829\n",
      "92/150 - loss 0.001; PSNR -inf; SSIM 0.853\n",
      "93/150 - loss 0.001; PSNR -inf; SSIM 0.796\n",
      "94/150 - loss 0.001; PSNR -inf; SSIM 0.771\n",
      "95/150 - loss 0.001; PSNR -inf; SSIM 0.828\n",
      "96/150 - loss 0.001; PSNR -inf; SSIM 0.853\n",
      "97/150 - loss 0.001; PSNR -inf; SSIM 0.767\n",
      "98/150 - loss 0.001; PSNR -inf; SSIM 0.801\n",
      "99/150 - loss 0.001; PSNR -inf; SSIM 0.838\n",
      "100/150 - loss 0.001; PSNR -inf; SSIM 0.799\n",
      "Saving Checkpoint...\n",
      "101/150 - loss 0.001; PSNR -inf; SSIM 0.840\n",
      "102/150 - loss 0.001; PSNR -inf; SSIM 0.854\n",
      "103/150 - loss 0.001; PSNR -inf; SSIM 0.858\n",
      "104/150 - loss 0.001; PSNR -inf; SSIM 0.856\n",
      "105/150 - loss 0.001; PSNR -inf; SSIM 0.864\n",
      "106/150 - loss 0.001; PSNR -inf; SSIM 0.875\n",
      "107/150 - loss 0.001; PSNR -inf; SSIM 0.868\n",
      "108/150 - loss 0.001; PSNR -inf; SSIM 0.857\n",
      "109/150 - loss 0.001; PSNR -inf; SSIM 0.849\n",
      "110/150 - loss 0.001; PSNR -inf; SSIM 0.866\n",
      "111/150 - loss 0.001; PSNR -inf; SSIM 0.873\n",
      "112/150 - loss 0.001; PSNR -inf; SSIM 0.886\n",
      "113/150 - loss 0.001; PSNR -inf; SSIM 0.867\n",
      "114/150 - loss 0.001; PSNR -inf; SSIM 0.872\n",
      "115/150 - loss 0.001; PSNR -inf; SSIM 0.880\n",
      "116/150 - loss 0.001; PSNR -inf; SSIM 0.855\n",
      "117/150 - loss 0.001; PSNR -inf; SSIM 0.864\n",
      "118/150 - loss 0.001; PSNR -inf; SSIM 0.891\n",
      "119/150 - loss 0.001; PSNR -inf; SSIM 0.884\n",
      "120/150 - loss 0.001; PSNR -inf; SSIM 0.893\n",
      "121/150 - loss 0.001; PSNR -inf; SSIM 0.879\n",
      "122/150 - loss 0.001; PSNR -inf; SSIM 0.894\n",
      "123/150 - loss 0.001; PSNR -inf; SSIM 0.897\n",
      "124/150 - loss 0.001; PSNR -inf; SSIM 0.878\n",
      "125/150 - loss 0.001; PSNR -inf; SSIM 0.886\n",
      "Saving Checkpoint...\n",
      "126/150 - loss 0.001; PSNR -inf; SSIM 0.883\n",
      "127/150 - loss 0.001; PSNR -inf; SSIM 0.886\n",
      "128/150 - loss 0.001; PSNR -inf; SSIM 0.888\n",
      "129/150 - loss 0.001; PSNR -inf; SSIM 0.894\n",
      "130/150 - loss 0.001; PSNR -inf; SSIM 0.893\n",
      "131/150 - loss 0.001; PSNR -inf; SSIM 0.893\n",
      "132/150 - loss 0.001; PSNR -inf; SSIM 0.902\n",
      "133/150 - loss 0.001; PSNR -inf; SSIM 0.882\n",
      "134/150 - loss 0.001; PSNR -inf; SSIM 0.897\n",
      "135/150 - loss 0.001; PSNR -inf; SSIM 0.904\n",
      "136/150 - loss 0.001; PSNR -inf; SSIM 0.899\n",
      "137/150 - loss 0.001; PSNR -inf; SSIM 0.899\n",
      "138/150 - loss 0.001; PSNR -inf; SSIM 0.895\n",
      "139/150 - loss 0.001; PSNR -inf; SSIM 0.903\n",
      "140/150 - loss 0.001; PSNR -inf; SSIM 0.897\n",
      "141/150 - loss 0.001; PSNR -inf; SSIM 0.905\n",
      "142/150 - loss 0.001; PSNR -inf; SSIM 0.899\n",
      "143/150 - loss 0.001; PSNR -inf; SSIM 0.900\n",
      "144/150 - loss 0.001; PSNR -inf; SSIM 0.904\n",
      "145/150 - loss 0.001; PSNR -inf; SSIM 0.903\n",
      "146/150 - loss 0.001; PSNR -inf; SSIM 0.907\n",
      "147/150 - loss 0.001; PSNR -inf; SSIM 0.907\n",
      "148/150 - loss 0.001; PSNR -inf; SSIM 0.906\n",
      "149/150 - loss 0.001; PSNR -inf; SSIM 0.909\n",
      "150/150 - loss 0.001; PSNR -inf; SSIM 0.906\n",
      "Saving Checkpoint...\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "scaler = GradScaler(growth_interval=10)\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    running_loss = 0.0\n",
    "    ssim_list = []\n",
    "    psnr_list = []\n",
    "    for idx, images in enumerate(train_loader, 0):\n",
    "        images = to_device(images, dev)\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            trainSino = generate_data(blur_image(images, kernel_size=5, sigma=2.0), proj, noise_level=np.random.uniform(0.1,1.2))\n",
    "        \n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "                \n",
    "            outputs = model(trainSino)\n",
    "\n",
    "            loss_value = loss_function(outputs, images)\n",
    "        \n",
    "        images = images.to('cpu').numpy()\n",
    "        del trainSino\n",
    "\n",
    "        scaler.scale(loss_value).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scale = scaler.get_scale()\n",
    "        scaler.update()\n",
    "\n",
    "        skip_lr_sched = (scale > scaler.get_scale())\n",
    "        if not skip_lr_sched:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        running_loss += loss_value.item()\n",
    "\n",
    "        for j in range(outputs.shape[0]):\n",
    "            for i in range(proj.in_shape[1]):\n",
    "                psnr_list.append(fom.psnr(outputs[j,:,i,:].detach().cpu().numpy(),images[j,:,i,:]))\n",
    "                ssim_list.append(fom.ssim(outputs[j,:,i,:].detach().cpu(),images[j,:,i,:]))\n",
    "        \n",
    "    running_loss /= len(train_loader)\n",
    "\n",
    "    psnr_value = np.mean(psnr_list)\n",
    "    ssim_value = np.mean(ssim_list)\n",
    "\n",
    "    print(\"{}/{} - loss {:.3f}; PSNR {:.3f}; SSIM {:.3f}\".format(\n",
    "        epoch+1, num_epochs, running_loss, psnr_value, ssim_value))\n",
    "    \n",
    "    if (epoch+1) % 25 == 0:\n",
    "        print(\"Saving Checkpoint...\")\n",
    "        cp = {'epoch': epoch,'model': model.module.state_dict(), 'optimizer': optimizer.state_dict(), 'lr_scheduler': lr_scheduler.state_dict()}\n",
    "        torch.save(cp, \"/home/mamo_alegua/anaconda3/LPDTransformer/\"+\"3iterTransformerDualV9Sino\"+str(epoch+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anton2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
